<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Early Pruning</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div id="page">
        <h1 id="main-title">Early Pruning</h1>
        <div id="article">
            Two recent, independent results in neural network sparsification has suggested
training smaller neural networks from scratch often out-performs training followed
by pruning, presenting a strong challenge to the body of work preceding it.  In
response,  a number of authors have challenged – or at least,  clarified – these
claims and offered their own empirical counter arguments. The narrative unfolding
represents a fascinating challenge (and anti-challenge) to more than two decades
of literature on the subject of network sparsification.  We offer new empirical
observations about the dynamics of discarding parameters earlier in the optimisation
procedure; our observations explore and expand upon recent dialogues in network
sparsification – offering new perspectives on and critical analyses of the existing
literature – as well as posing new questions to the community that, we believe, are
indicative of the dominant factors at play in the cases considered.
        </div>
    </div>
</body>
</html>
